{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 50\n",
    "THRESHOLD = 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, env_string,batch_size=64):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make(env_string)\n",
    "        input_size = self.env.observation_space.shape[0]\n",
    "        self.action = np.array([-2,0,2], dtype=np.float32)\n",
    "        action_size = len(self.action)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = 1.0\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.1\n",
    "        \n",
    "        alpha=0.01\n",
    "        alpha_decay=0.01\n",
    "        \n",
    "        # Init model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(64, input_dim=input_size, activation='relu'))\n",
    "        self.model.add(Dense(32, activation='relu'))\n",
    "        # self.model.add(Dense(action_size, activation='linear'))\n",
    "        self.model.add(Dense(3, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=alpha))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return [np.random.choice(self.action)]\n",
    "        else:\n",
    "            return [self.action[np.argmax(self.model.predict(state, verbose=0)[0])]]\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 3])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state, verbose=0)\n",
    "            action_index = np.where(self.action == action[0])[0][0]\n",
    "            y_target[0][action_index] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state, verbose=0)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_decay*self.epsilon) # decrease epsilon\n",
    "       \n",
    "\n",
    "    def train(self):\n",
    "        scores = deque(maxlen=100)\n",
    "        avg_scores = []\n",
    "\n",
    "        for e in range(EPISODES):\n",
    "            state = self.env.reset()\n",
    "            if e % 10 == 0:\n",
    "                self.env.render()\n",
    "            state = self.preprocess_state(state)\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state,self.epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                if e % 10 == 0:\n",
    "                    self.env.render()\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                self.epsilon = max(self.epsilon_min, self.epsilon_decay*self.epsilon) # decrease epsilon\n",
    "                i += 1\n",
    "                \n",
    "            if e % 10 == 0:\n",
    "                self.env.close()\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            avg_scores.append(mean_score)\n",
    "            print(f'Epoch number {e}, mean score {mean_score}, reward {reward}')\n",
    "            if mean_score >= THRESHOLD and e >= 10:\n",
    "                print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 10))\n",
    "                return avg_scores\n",
    "            if (e + 1) % 10 == 0:\n",
    "                print('[Episode {}] - Mean survival time over last 10 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0, mean score 200.0, reward -5.342642604018294\n",
      "Epoch number 1, mean score 200.0, reward -9.592861337153957\n",
      "Epoch number 2, mean score 200.0, reward -8.601326385805423\n",
      "Epoch number 3, mean score 200.0, reward -5.034676835784575\n",
      "Epoch number 4, mean score 200.0, reward -7.604488991258211\n",
      "Epoch number 5, mean score 200.0, reward -8.143848878274552\n",
      "Epoch number 6, mean score 200.0, reward -8.50809972489631\n",
      "Epoch number 7, mean score 200.0, reward -8.561474142973045\n",
      "Epoch number 8, mean score 200.0, reward -9.412909600106607\n",
      "Epoch number 9, mean score 200.0, reward -9.8518839581704\n",
      "[Episode 9] - Mean survival time over last 10 episodes was 200.0 ticks.\n",
      "Epoch number 10, mean score 200.0, reward -9.87091409769772\n",
      "Epoch number 11, mean score 200.0, reward -9.701976814305565\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m env_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPendulum-v0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m dqn \u001b[38;5;241m=\u001b[39m DQN(env_string)\n\u001b[1;32m----> 3\u001b[0m avg_scores \u001b[38;5;241m=\u001b[39m dqn\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming avg_scores is the list returned from the train method\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_avg_reward\u001b[39m(avg_scores):\n",
      "Cell \u001b[1;32mIn[7], line 86\u001b[0m, in \u001b[0;36mDQN.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (e \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Episode \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] - Mean survival time over last 10 episodes was \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m ticks.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e, mean_score))\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDid not solve after \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m episodes ðŸ˜ž\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg_scores\n",
      "Cell \u001b[1;32mIn[7], line 41\u001b[0m, in \u001b[0;36mDQN.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     39\u001b[0m minibatch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory), batch_size))\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state, action, reward, next_state, done \u001b[38;5;129;01min\u001b[39;00m minibatch:\n\u001b[1;32m---> 41\u001b[0m     y_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(state, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     42\u001b[0m     action_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m==\u001b[39m action[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     43\u001b[0m     y_target[\u001b[38;5;241m0\u001b[39m][action_index] \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(next_state, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\adamt\\anaconda3\\envs\\RL\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\adamt\\anaconda3\\envs\\RL\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:505\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    503\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    506\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m    507\u001b[0m         data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n",
      "File \u001b[1;32mc:\\Users\\adamt\\anaconda3\\envs\\RL\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:649\u001b[0m, in \u001b[0;36mTFEpochIterator.enumerate_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 649\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distributed_dataset)\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution\n\u001b[0;32m    653\u001b[0m         ):\n",
      "File \u001b[1;32mc:\\Users\\adamt\\anaconda3\\envs\\RL\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iterator_ops\u001b[38;5;241m.\u001b[39mOwnedIterator(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adamt\\anaconda3\\envs\\RL\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_iterator(dataset)\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\adamt\\anaconda3\\envs\\RL\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmake_iterator(ds_variant, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource)\n",
      "File \u001b[1;32mc:\\Users\\adamt\\anaconda3\\envs\\RL\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3509\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3508\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3509\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m   3510\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMakeIterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, dataset, iterator)\n\u001b[0;32m   3511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3512\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_string = 'Pendulum-v0'\n",
    "\n",
    "def plot_avg_reward(avg_scores):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(avg_scores, label='Average Reward per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Average Reward per Episode over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "dqn = DQN(env_string)\n",
    "avg_scores = dqn.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN():\n",
    "    def __init__(self, env_string, batch_size=64):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make(env_string)\n",
    "        input_size = self.env.observation_space.shape[0]\n",
    "        self.action = np.array([-2, 0, 2], dtype=np.float32)\n",
    "        action_size = len(self.action)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = 1.0\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        alpha = 0.01\n",
    "        alpha_decay = 0.01\n",
    "        \n",
    "        # Init model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(64, input_dim=input_size, activation='relu'))\n",
    "        self.model.add(Dense(32, activation='relu'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=alpha))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return [np.random.choice(self.action)]\n",
    "        else:\n",
    "            return [self.action[np.argmax(self.model.predict(state, verbose=0)[0])]]\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 3])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state, verbose=0)\n",
    "            action_index = np.where(self.action == action[0])[0][0]\n",
    "            y_target[0][action_index] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state, verbose=0)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)  # decrease epsilon\n",
    "\n",
    "    def train(self, EPISODES, THRESHOLD):\n",
    "        scores = deque(maxlen=100)\n",
    "        rewards = []\n",
    "        average_rewards = []\n",
    "\n",
    "        for e in range(EPISODES):\n",
    "            state = self.env.reset()\n",
    "            if e % 10 == 0:\n",
    "                self.env.render()\n",
    "            state = self.preprocess_state(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state, self.epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                if e % 10 == 0:\n",
    "                    self.env.render()\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            if e % 10 == 0:\n",
    "                self.env.close()\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "            scores.append(total_reward)\n",
    "            mean_score = np.mean(scores)\n",
    "            average_rewards.append(mean_score)\n",
    "            print(f'Epoch number {e}, mean score {mean_score}, reward {total_reward}')\n",
    "            if mean_score >= THRESHOLD and e >= 10:\n",
    "                print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 10))\n",
    "                return rewards, average_rewards\n",
    "            if (e + 1) % 10 == 0:\n",
    "                print('[Episode {}] - Mean survival time over last 10 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return rewards, average_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0, mean score -1732.2229243318654, reward -1732.2229243318654\n",
      "Epoch number 1, mean score -1726.0153517576973, reward -1719.807779183529\n",
      "Epoch number 2, mean score -1511.2336432160764, reward -1081.6702261328344\n",
      "Epoch number 3, mean score -1545.4887255872036, reward -1648.253972700585\n",
      "Epoch number 4, mean score -1438.90476230872, reward -1012.5689091947852\n",
      "Epoch number 5, mean score -1377.0684258631618, reward -1067.886743635371\n",
      "Epoch number 6, mean score -1411.474122577089, reward -1617.9083028606537\n",
      "Epoch number 7, mean score -1349.1442079860437, reward -912.834805848725\n",
      "Epoch number 8, mean score -1305.8753571281004, reward -959.7245502645521\n",
      "Epoch number 9, mean score -1250.095931575663, reward -748.081101603726\n",
      "[Episode 9] - Mean survival time over last 10 episodes was -1250.095931575663 ticks.\n",
      "Epoch number 10, mean score -1294.0915540943074, reward -1734.0477792807521\n",
      "Epoch number 11, mean score -1275.9716997167748, reward -1076.6533015639168\n",
      "Epoch number 12, mean score -1260.5604922932353, reward -1075.6260032107625\n",
      "Epoch number 13, mean score -1244.6980829345944, reward -1038.4867612722644\n",
      "Epoch number 14, mean score -1251.8102809559637, reward -1351.3810532551336\n",
      "Epoch number 15, mean score -1241.0886477925278, reward -1080.264150340986\n",
      "Epoch number 16, mean score -1218.5977182322529, reward -858.7428452678539\n",
      "Epoch number 17, mean score -1203.6595107325975, reward -949.709983238454\n",
      "Epoch number 18, mean score -1192.609948688775, reward -993.7178318999675\n",
      "Epoch number 19, mean score -1176.7704708432489, reward -875.8203917782505\n",
      "[Episode 19] - Mean survival time over last 10 episodes was -1176.7704708432489 ticks.\n",
      "Epoch number 20, mean score -1184.4538968083257, reward -1338.1224161098626\n",
      "Epoch number 21, mean score -1188.111407185162, reward -1264.9191250987262\n",
      "Epoch number 22, mean score -1171.943108338983, reward -816.240533723046\n",
      "Epoch number 23, mean score -1167.780829987267, reward -1072.0484278978088\n",
      "Epoch number 24, mean score -1156.596134157794, reward -888.163434250445\n",
      "Epoch number 25, mean score -1155.2636563726612, reward -1121.9517117443368\n",
      "Epoch number 26, mean score -1170.6694391827737, reward -1571.2197922456978\n",
      "Epoch number 27, mean score -1171.341858817739, reward -1189.4971889618007\n",
      "Epoch number 28, mean score -1174.9512449658623, reward -1276.0140571133136\n",
      "Epoch number 29, mean score -1183.0867022163295, reward -1419.0149624798808\n",
      "[Episode 29] - Mean survival time over last 10 episodes was -1183.0867022163295 ticks.\n",
      "Epoch number 30, mean score -1194.4334692734942, reward -1534.8364809884351\n",
      "Epoch number 31, mean score -1210.0633654225885, reward -1694.5901460445086\n",
      "Epoch number 32, mean score -1213.8000005794972, reward -1333.3723256005758\n",
      "Epoch number 33, mean score -1206.422451623607, reward -962.9633360792261\n",
      "Epoch number 34, mean score -1200.0074702175127, reward -981.8981024103056\n",
      "Epoch number 35, mean score -1203.4993362494, reward -1325.7146473654586\n",
      "Epoch number 36, mean score -1212.1902428696942, reward -1525.0628812002867\n",
      "Epoch number 37, mean score -1212.561620967924, reward -1226.302610602429\n",
      "Epoch number 38, mean score -1225.3071190204614, reward -1709.6360450168881\n",
      "Epoch number 39, mean score -1231.058087010028, reward -1455.3458386031186\n",
      "[Episode 39] - Mean survival time over last 10 episodes was -1231.058087010028 ticks.\n",
      "Epoch number 40, mean score -1243.7570256553352, reward -1751.7145714676203\n",
      "Epoch number 41, mean score -1250.7149060525019, reward -1535.9880023363382\n",
      "Epoch number 42, mean score -1253.1547482732315, reward -1355.6281215438723\n",
      "Epoch number 43, mean score -1259.14238252712, reward -1516.6106554443218\n",
      "Epoch number 44, mean score -1269.0234338233593, reward -1703.7896908578912\n",
      "Epoch number 45, mean score -1272.0652536624282, reward -1408.9471464205278\n",
      "Epoch number 46, mean score -1274.669843430292, reward -1394.4809727520362\n",
      "Epoch number 47, mean score -1279.187624786735, reward -1491.523348539547\n",
      "Epoch number 48, mean score -1289.4075355741552, reward -1779.963253370328\n",
      "Epoch number 49, mean score -1280.954019684301, reward -866.7317410814468\n",
      "[Episode 49] - Mean survival time over last 10 episodes was -1280.954019684301 ticks.\n",
      "Did not solve after 49 episodes ðŸ˜ž\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'figure'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m dqn \u001b[38;5;241m=\u001b[39m DQN(env_string)\n\u001b[0;32m     26\u001b[0m rewards, average_rewards \u001b[38;5;241m=\u001b[39m dqn\u001b[38;5;241m.\u001b[39mtrain(EPISODES, THRESHOLD)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mplot_rewards\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_rewards\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mplot_rewards\u001b[1;34m(rewards, average_rewards)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_rewards\u001b[39m(rewards, average_rewards):\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(rewards, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward per Episode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(average_rewards, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Reward per Episode\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\_api\\__init__.py:222\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance)\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'figure'"
     ]
    }
   ],
   "source": [
    "def plot_avg_reward(avg_scores):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(avg_scores, label='Average Reward per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Average Reward per Episode over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_rewards(rewards, average_rewards):\n",
    "    # plt.fig(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(rewards, label='Reward per Episode')\n",
    "    plt.plot(average_rewards, label='Average Reward per Episode', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reward and Average Reward per Episode')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "env_string = 'Pendulum-v0'\n",
    "dqn = DQN(env_string)\n",
    "rewards, average_rewards = dqn.train(EPISODES, THRESHOLD)\n",
    "plot_rewards(rewards, average_rewards)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
