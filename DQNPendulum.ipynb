{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "THRESHOLD = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, env_string,batch_size=64):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make(env_string)\n",
    "        input_size = self.env.observation_space.shape[0]\n",
    "        self.action = np.array([-2,0,2], dtype=np.float32)\n",
    "        action_size = len(self.action)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = 1.0\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        alpha=0.01\n",
    "        alpha_decay=0.01\n",
    "        \n",
    "        # Init model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim=input_size, activation='tanh'))\n",
    "        self.model.add(Dense(48, activation='tanh'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=alpha))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return [np.random.choice(self.action)]\n",
    "        else:\n",
    "            return [self.action[np.argmax(self.model.predict(state, verbose=0)[0])]]\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 3])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state, verbose=0)\n",
    "            action_index = np.where(self.action == action[0])[0][0]\n",
    "            y_target[0][action_index] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state, verbose=0)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        #epsilon = max(epsilon_min, epsilon_decay*epsilon) # decrease epsilon\n",
    "       \n",
    "\n",
    "    def train(self):\n",
    "        scores = deque(maxlen=100)\n",
    "        avg_scores = []\n",
    "        \n",
    "\n",
    "        for e in range(EPOCHS):\n",
    "            state = self.env.reset()\n",
    "            if e % 10 == 0:\n",
    "                self.env.render()\n",
    "            state = self.preprocess_state(state)\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state,self.epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                if e % 10 == 0:\n",
    "                    self.env.render()\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                self.epsilon = max(self.epsilon_min, self.epsilon_decay*self.epsilon) # decrease epsilon\n",
    "                i += 1\n",
    "                \n",
    "            if e % 10 == 0:\n",
    "                self.env.close()\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            avg_scores.append(mean_score)\n",
    "            print(f'Epoch number {e}')\n",
    "            if mean_score >= THRESHOLD and e >= 10:\n",
    "                print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 10))\n",
    "                return avg_scores\n",
    "            if (e + 1) % 10 == 0:\n",
    "                print('[Episode {}] - Mean survival time over last 10 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p2300575\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      "Epoch number 1\n",
      "Epoch number 2\n",
      "Epoch number 3\n",
      "Epoch number 4\n",
      "Epoch number 5\n",
      "Epoch number 6\n",
      "Epoch number 7\n",
      "Epoch number 8\n",
      "Epoch number 9\n",
      "[Episode 9] - Mean survival time over last 10 episodes was 200.0 ticks.\n",
      "Epoch number 10\n",
      "Epoch number 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m env_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPendulum-v0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m dqn \u001b[38;5;241m=\u001b[39m DQN(env_string)\n\u001b[1;32m----> 3\u001b[0m avg_scores \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36mDQN.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (e \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Episode \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] - Mean survival time over last 10 episodes was \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m ticks.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e, mean_score))\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDid not solve after \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m episodes ðŸ˜ž\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg_scores\n",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36mDQN.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     38\u001b[0m minibatch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory), batch_size))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state, action, reward, next_state, done \u001b[38;5;129;01min\u001b[39;00m minibatch:\n\u001b[1;32m---> 40\u001b[0m     y_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     action_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m==\u001b[39m action[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     42\u001b[0m     y_target[\u001b[38;5;241m0\u001b[39m][action_index] \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(next_state, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:505\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    503\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    506\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m    507\u001b[0m         data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:649\u001b[0m, in \u001b[0;36mTFEpochIterator.enumerate_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 649\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution\n\u001b[0;32m    653\u001b[0m         ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3509\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3508\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3509\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3510\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3512\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_string = 'Pendulum-v0'\n",
    "dqn = DQN(env_string)\n",
    "avg_scores = dqn.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
