{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DELE ST1504 CA2\n",
        "# PART A: GAN\n",
        "\n",
        "<hr>\n",
        "\n",
        "**NAME**: Irman Zafyree, Adam Tan\n",
        "\n",
        "**ADMIN NO**: `2300546`, `2300575`\n",
        "\n",
        "**CLASS**: DAAA/FT/2B/07\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Objective:**\n",
        "\n",
        "Code a GAN model that is able to generate 260 small black-and-white images of the given dataset in 26 distinct classes.\n",
        "\n",
        "**Background:**\n",
        "A Generative Adversarial Network (GAN) is a type of deep learning model consisting of two neural networks: a generator and a discriminator. The primary purpose of a GAN is to generate new data instances that resemble the training data, through a competitive process between a generator and a discriminator. It has revolutionized the field of generative modeling and continues to be a vibrant area of research and application in artificial intelligence.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic imports\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from IPython import display\n",
        "\n",
        "# Tensorflow imports\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.layers import Dense, Reshape, Conv2D, LeakyReLU, Dropout, Flatten, Conv2DTranspose, BatchNormalization\n",
        "from tensorflow.train import Checkpoint\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "dataset_path = os.environ.get(\"DATASET_PATH\")\n",
        "\n",
        "print(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    print(tf.config.experimental.get_device_details(gpu))\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "print(f\"There are {len(gpus)} GPUs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_path, header=None)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Background Research\n",
        "\n",
        "**CSV Dataset:**\n",
        "- the datatset consists of 99040 datasets\n",
        "- the first column represents the classs labels -1 to 26.\n",
        "- Remaining 784 columns are the pixel data\n",
        "\n",
        "**Classes:**\n",
        "\n",
        "Total of 27 distinct classes \n",
        "1. -1 (all black)\n",
        "2. 1 (a)\n",
        "3. 2 (b)\n",
        "4. 3 (c)\n",
        "5. 4 (d)\n",
        "6. 5 (e)\n",
        "7. 6 (f)\n",
        "8. 7 (g)\n",
        "9. 8 (h)\n",
        "10. 9 (i)\n",
        "11. 10 (j)\n",
        "12. 11 (k)\n",
        "13. 12 (l)\n",
        "14. 13 (m)\n",
        "15. 14 (n)\n",
        "16. 15 (o)\n",
        "17. 16 (p)\n",
        "18. 17 (q)\n",
        "19. 18 (r)\n",
        "20. 19 (s)\n",
        "21. 20 (t)\n",
        "22. 21 (u)\n",
        "23. 22 (v)\n",
        "24. 23 (w)\n",
        "25. 24 (x)\n",
        "26. 25 (y)\n",
        "27. 26 (z)\n",
        "\n",
        "\n",
        "**Images:**\n",
        "\n",
        "The images are of size 28x28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = df[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df.drop(df.columns[0], axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Function to display images\n",
        "def display_images(images, label):\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for index, image in enumerate(images[:20]):\n",
        "        plt.subplot(1, 20, index + 1)\n",
        "        image = np.array(image).reshape(28, 28)\n",
        "        rotated_image = np.rot90(image, k=-1)  \n",
        "        flipped_horizontal = np.fliplr(rotated_image)\n",
        "        plt.imshow(flipped_horizontal, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{label}\")\n",
        "\n",
        "# Order labels from -1 to 26\n",
        "ordered_labels = sorted(labels.unique(), key=lambda x: (x != -1, x))\n",
        "\n",
        "# Iterate through each ordered label and display 10 images\n",
        "for label in ordered_labels:\n",
        "    label_images = data[labels == label].values\n",
        "    display_images(label_images, label)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Count occurrences of each label\n",
        "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "# Print the distribution\n",
        "print(\"Class distribution:\")\n",
        "for label, count in zip(unique_labels, label_counts):\n",
        "    print(f\"Label {label}: {count} instances\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))  # Specify the figure size (optional)\n",
        "plt.bar(unique_labels, label_counts, color='skyblue')\n",
        "\n",
        "# Customize the chart\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Counts')\n",
        "plt.grid(True)  # Add gridlines for better readability (optional)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df[df[0] != -1]\n",
        "df.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_labels = df[0]\n",
        "df_data = df.drop(df.columns[0:2], axis=1)\n",
        "df_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "augmented_data = df_data.copy()\n",
        "for i in augmented_data.index:\n",
        "    pixels = augmented_data.loc[i].values\n",
        "    image = np.array(pixels).reshape(28, 28)\n",
        "    rotated_image = np.rot90(image, k=-1)  \n",
        "    flipped_horizontal = np.fliplr(rotated_image)\n",
        "    augmented_data.loc[i] = flipped_horizontal.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(np.array(augmented_data.loc[i]).reshape(28, 28), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_images_per_class(data, labels):\n",
        "\n",
        "    avg_image_df = pd.DataFrame()\n",
        "\n",
        "    # average image\n",
        "    for class_name, group in data.groupby(labels):\n",
        "        avg_image = group.mean()\n",
        "        avg_image_df[class_name] = avg_image\n",
        "\n",
        "    # Create a col of subplots\n",
        "    fig, axes = plt.subplots(5, 6, figsize=(20, 15))\n",
        "    for ax, (class_name, avg_image) in zip(axes.flatten(), avg_image_df.items()):\n",
        "        # image = avg_image.reshape(28, 28)\n",
        "        # rotated_image = np.rot90(image, k=-1)  \n",
        "        # flipped_horizontal = np.fliplr(rotated_image)\n",
        "        # ax.imshow(flipped_horizontal, cmap='gray')\n",
        "        # ax.set_title(f\"Average image for {class_name}\")\n",
        "        # ax.axis('off')  # Turn off axis\n",
        "\n",
        "        ax.imshow(np.array(avg_image).reshape(28, 28), cmap='gray')\n",
        "        ax.set_title(f\"Average image for {class_name}\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "average_images_per_class(augmented_data, df_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Research\n",
        "\n",
        "From researching online, there seem to be 3 GAN Variations, Deep Convolutional GAN (DCGAN), Conditional GAN (CGAN), Wasserstein GAN (WGAN).\n",
        "\n",
        "### Deep Convolutional GAN (DCGAN)\n",
        "\n",
        "DCGANs use deep convolutional neural networks to learn the features of the input data, which allows them to generate high-resolution images that are similar to the training data. The generator network in a DCGAN typically consists of transposed convolutional layers, while the discriminator network consists of convolutional layers. The use of convolutional layers allows DCGANs to take advantage of the spatial relationships in the input data, which results in high-quality generated images.\n",
        "\n",
        "### Conditional GAN (CGAN)\n",
        "\n",
        "CGANs that allow the user to control the generated data by adding an additional input to the generator network. This input specifies the desired characteristics of the generated data, such as the color or shape of an image. CGANs can be thought of as a combination of a GAN and a conditional generative model, where the generator and discriminator are trained to take into account both the input data and the condition. This allows CGANs to generate data that is more in line with the user’s expectations.\n",
        "\n",
        "### Wasserstein GAN (WGAN)\n",
        "\n",
        "WGANs address some of the stability issues that are commonly encountered when training GANs. WGANs use the Wasserstein distance metric to evaluate the quality of the generated data, which has been shown to provide improved stability during training compared to other metrics. The Wasserstein distance metric measures the earth mover’s distance between the generated data and the training data, which provides a robust measure of the quality of the generated data. WGANs have been shown to be particularly effective for generating high-quality images.\n",
        "\n",
        "We will be using all these 3 GAN and compare them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background on GAN\n",
        "\n",
        "A generative adversarial network (GAN) has two parts:\n",
        "- Generator\n",
        "  - learns to create \"real\" data\n",
        "  - generated data becomes the fake training examples\n",
        "- Discriminator\n",
        "  - distinguish between real and fake data\n",
        "  - penalizes the generator if discriminator detects the fake generated data\n",
        "\n",
        "When the training begins, the generator fake data is easily detectable by the discriminator. However, if the generator training goes well, the fake data becomes indistinguishable to the discriminator.\n",
        "\n",
        "Below is an example of how the generator improves over time:\n",
        "1. ![Beginning](https://developers.google.com/static/machine-learning/gan/images/bad_gan.svg)\n",
        "2. ![Middle](https://developers.google.com/static/machine-learning/gan/images/ok_gan.svg)\n",
        "3. ![End](https://developers.google.com/static/machine-learning/gan/images/good_gan.svg)\n",
        "\n",
        "So this is how the entire system will look.\n",
        "![Entire GAN system](https://developers.google.com/static/machine-learning/gan/images/gan_diagram.svg)\n",
        "\n",
        "### Discriminator\n",
        "The main goal of the discriminator is to distinguish real data from the fake generated data. It could use any network architecture appropriate to the type of data it's classifying.\n",
        "\n",
        "The discriminator training data come from 2 sources.\n",
        "- Real data comes from the initial dataset that you would like to generate like, in this case, it would be out `emnist-letter.csv`\n",
        "- Fake data comes from the generator.\n",
        "\n",
        "The discriminator trains by:\n",
        "1. Classifying real and fake\n",
        "2. The loss penalizes the discriminator for misclassifying\n",
        "3. The discriminator updates it weights through backpropagation\n",
        "\n",
        "### Generator\n",
        "The main goal of the generator is to trick the discriminator into thinking it fake data is real.\n",
        "\n",
        "The generator has a random input. It is usually random noise, and the generator turns the noise into meaningful data. Experiments suggest that the distribution of the noise doesn't matter much, so we can choose something that's easy to sample from, like a uniform distribution.\n",
        "\n",
        "The generators trains by:\n",
        "1. Sample noise\n",
        "2. Produce output\n",
        "3. Get discriminator to classify\n",
        "4. Get the loss from the discriminator\n",
        "5. Backpropagate through the generator network\n",
        "6. Updates the weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DCGAN:\n",
        "    def __init__(self, noise_dim, BATCH_SIZE, input_shape=(28, 28, 1)):\n",
        "        self.noise_dim = noise_dim\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "        self.input_shape = input_shape\n",
        "        self.generator = self.make_generator_model()\n",
        "        self.discriminator = self.make_discriminator_model()\n",
        "        self.generator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
        "        self.discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
        "        self.loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        self.checkpoint_dir = './training_checkpoints'\n",
        "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, \"ckpt\")\n",
        "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=self.generator_optimizer,\n",
        "                                              discriminator_optimizer=self.discriminator_optimizer,\n",
        "                                              generator=self.generator,\n",
        "                                              discriminator=self.discriminator)\n",
        "        self.num_examples_to_generate = 16\n",
        "        self.seed = tf.random.normal([self.num_examples_to_generate, noise_dim])\n",
        "        self.logs = {\n",
        "            \"generator_loss\": [],\n",
        "            \"discriminator_loss\": [],\n",
        "            \"KL_divergence\": [],\n",
        "        }\n",
        "\n",
        "    def make_generator_model(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            layers.Dense(7*7*256, use_bias=False, input_shape=(self.noise_dim,)),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.LeakyReLU(),\n",
        "            layers.Reshape((7, 7, 256)),\n",
        "            layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.LeakyReLU(),\n",
        "            layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.LeakyReLU(),\n",
        "            layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def make_discriminator_model(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]),\n",
        "            layers.LeakyReLU(),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
        "            layers.LeakyReLU(),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(1)\n",
        "        ])\n",
        "        return model\n",
        "    \n",
        "    def discriminator_loss(self, real_output, fake_output):\n",
        "        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output)\n",
        "        fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
        "        total_loss = real_loss + fake_loss\n",
        "        return total_loss\n",
        "\n",
        "    def generator_loss(self, fake_output):\n",
        "        return tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "    def kl_divergence(self, real_data, generated_data):\n",
        "        epsilon = 1e-10\n",
        "        real_data_hist = tf.histogram_fixed_width(real_data, [0, 1], nbins=256)\n",
        "        generated_data_hist = tf.histogram_fixed_width(generated_data, [0, 1], nbins=256)\n",
        "        real_data_prob = real_data_hist / tf.reduce_sum(real_data_hist)\n",
        "        generated_data_prob = generated_data_hist / tf.reduce_sum(generated_data_hist)\n",
        "        epsilon = 1e-10\n",
        "        real_data_prob += epsilon\n",
        "        generated_data_prob += epsilon\n",
        "        kl_div = tf.reduce_sum(real_data_prob * tf.math.log(real_data_prob / generated_data_prob+epsilon))\n",
        "        return kl_div\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, images):\n",
        "        # noise = tf.random.normal([self.BATCH_SIZE, self.noise_dim])\n",
        "\n",
        "        # with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        #     generated_images = self.generator(noise, training=True)\n",
        "\n",
        "        #     real_output = self.discriminator(images, training=True)\n",
        "        #     fake_output = self.discriminator(generated_images, training=True)\n",
        "\n",
        "        #     gen_loss = self.generator_loss(fake_output)\n",
        "        #     disc_loss = self.discriminator_loss(real_output, fake_output)\n",
        "\n",
        "        # # Log the loss values here\n",
        "        # kl_divergence = self.kl_divergence(real_output, fake_output)\n",
        "        # self.logs[\"generator_loss\"].append(gen_loss)\n",
        "        # self.logs[\"discriminator_loss\"].append(disc_loss)\n",
        "        # self.logs[\"KL_divergence\"].append(kl_divergence)\n",
        "\n",
        "        # gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "        # gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        # self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
        "        # self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
        "\n",
        "        noise = tf.random.normal([self.BATCH_SIZE, self.noise_dim])\n",
        "        generated_images = self.generator(noise, training=True)\n",
        "        combined_images = tf.concat([images, generated_images], axis=0)\n",
        "        labels = tf.concat([tf.ones((self.BATCH_SIZE, 1)), tf.zeros((self.BATCH_SIZE, 1))], axis=0)\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images, training=True)\n",
        "            print(predictions.shape)\n",
        "            print(labels.shape)\n",
        "            d_loss = self.loss_function(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.discriminator_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
        "        misleading_labels = tf.zeros((self.BATCH_SIZE, 1))\n",
        "        with tf.GradientTape() as tape:\n",
        "            generated_images = self.generator(noise, training=True)\n",
        "            predictions = self.discriminator(generated_images, training=True)\n",
        "            g_loss = self.loss_function(misleading_labels, predictions)\n",
        "            kl_div = self.kl_divergence(images, generated_images)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.generator_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "    def train(self, dataset, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            for image_batch in tqdm(dataset, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
        "                self.train_step(image_batch)\n",
        "\n",
        "            # Produce images for the GIF as we go\n",
        "            # display.clear_output(wait=True)\n",
        "            self.generate_and_save_images(epoch + 1, self.seed)\n",
        "\n",
        "            # Save the model every 15 epochs\n",
        "            if (epoch + 1) % 15 == 0:\n",
        "                self.checkpoint.save(file_prefix = self.checkpoint_prefix)\n",
        "\n",
        "        # Generate after the final epoch\n",
        "        # display.clear_output(wait=True)\n",
        "        self.generate_and_save_images(epochs, self.seed)\n",
        "        self.generate_plots()\n",
        "\n",
        "    def generate_plots(self):\n",
        "        print(self.logs)\n",
        "        # plt.figure(figsize=(20, 5))\n",
        "        # plt.subplot(1, 3, 1)\n",
        "        # generator_loss_values = [loss.numpy() for loss in self.logs[\"generator_loss\"]]\n",
        "        # print(generator_loss_values)\n",
        "        # discriminator_loss_values = [loss.numpy() for loss in self.logs[\"discriminator_loss\"]]\n",
        "        # print(discriminator_loss_values)\n",
        "        # plt.plot(generator_loss_values, label=\"Generator Loss\")\n",
        "        # plt.plot(discriminator_loss_values, label=\"Discriminator Loss\")\n",
        "        # plt.title(\"Losses\")\n",
        "        # plt.legend()\n",
        "\n",
        "        # plt.subplot(1, 3, 2)\n",
        "        # plt.plot(self.logs[\"KL_divergence\"], label=\"KL Divergence\")\n",
        "        # plt.title(\"KL Divergence\")\n",
        "        # plt.legend()\n",
        "\n",
        "        # plt.show()\n",
        "\n",
        "    def generate_and_save_images(self, epoch, test_input):\n",
        "        predictions = self.generator(test_input, training=False)\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "        for i in range(predictions.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "        imageDir = './images'\n",
        "        if not os.path.exists(imageDir):\n",
        "            os.makedirs(imageDir)\n",
        "        imagePath = os.path.join(imageDir, 'image_at_epoch_{:04d}.png'.format(epoch))\n",
        "        plt.savefig(imagePath)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_path, header=None)\n",
        "\n",
        "train_labels = df[0]\n",
        "train_data = df.drop(0, axis=1)\n",
        "augmented_data = []\n",
        "for i in train_data.index:\n",
        "    pixels = train_data.loc[i].values\n",
        "    image = np.array(pixels).reshape(28,28)\n",
        "    rotated_image = np.rot90(image, k=-1)\n",
        "    flipped_horizontal = np.fliplr(rotated_image)\n",
        "    augmented_data.append(flipped_horizontal)\n",
        "\n",
        "train_data = np.array(augmented_data)\n",
        "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1).astype('float32')\n",
        "train_data = (train_data - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
        "\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3:   0%|          | 0/387 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(512, 1)\n",
            "(512, 1)\n",
            "(512, 1)\n",
            "(512, 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|█████████▉| 386/387 [00:09<00:00, 40.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(480, 1)\n",
            "(512, 1)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\p2300575\\AppData\\Local\\Temp\\ipykernel_4736\\2248123887.py\", line 109, in train_step  *\n        d_loss = self.loss_function(labels, predictions)\n    File \"c:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__  **\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 2162, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\backend.py\", line 5677, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((480, 1) vs (512, 1)).\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[81], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dcgan \u001b[38;5;241m=\u001b[39m DCGAN(\u001b[38;5;241m100\u001b[39m, BATCH_SIZE, (\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdcgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[80], line 124\u001b[0m, in \u001b[0;36mDCGAN.train\u001b[1;34m(self, dataset, epochs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 124\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m# Produce images for the GIF as we go\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m# display.clear_output(wait=True)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_and_save_images(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n",
            "File \u001b[1;32mc:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexrkp15pt.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     16\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(ag__\u001b[38;5;241m.\u001b[39mld(predictions)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     17\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(ag__\u001b[38;5;241m.\u001b[39mld(labels)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 18\u001b[0m     d_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mloss_function, (ag__\u001b[38;5;241m.\u001b[39mld(labels), ag__\u001b[38;5;241m.\u001b[39mld(predictions)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     19\u001b[0m grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(d_loss), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mtrainable_weights), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     20\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdiscriminator_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(grads), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mtrainable_weights), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
            "File \u001b[1;32mc:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py:152\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 152\u001b[0m losses \u001b[38;5;241m=\u001b[39m call_fn(y_true, y_pred)\n\u001b[0;32m    153\u001b[0m mask \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39mget_mask(losses)\n\u001b[0;32m    154\u001b[0m reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reduction()\n",
            "File \u001b[1;32mc:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py:272\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    265\u001b[0m     y_pred, y_true \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39msqueeze_or_expand_dimensions(\n\u001b[0;32m    266\u001b[0m         y_pred, y_true\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    269\u001b[0m ag_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[0;32m    271\u001b[0m )\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mag_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py:2162\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[0;32m   2155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_true \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m label_smoothing) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m label_smoothing\n\u001b[0;32m   2157\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39msmart_cond\u001b[38;5;241m.\u001b[39msmart_cond(\n\u001b[0;32m   2158\u001b[0m     label_smoothing, _smooth_labels, \u001b[38;5;28;01mlambda\u001b[39;00m: y_true\n\u001b[0;32m   2159\u001b[0m )\n\u001b[0;32m   2161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m-> 2162\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_crossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_logits\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   2163\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   2164\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\backend.py:5677\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[1;34m(target, output, from_logits)\u001b[0m\n\u001b[0;32m   5673\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[0;32m   5674\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5675\u001b[0m )\n\u001b[0;32m   5676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n\u001b[1;32m-> 5677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\n\u001b[0;32m   5679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5681\u001b[0m epsilon_ \u001b[38;5;241m=\u001b[39m _constant_to_tensor(epsilon(), output\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype)\n\u001b[0;32m   5682\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mclip_by_value(output, epsilon_, \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon_)\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\p2300575\\AppData\\Local\\Temp\\ipykernel_4736\\2248123887.py\", line 109, in train_step  *\n        d_loss = self.loss_function(labels, predictions)\n    File \"c:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__  **\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 2162, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\p2300575\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\backend.py\", line 5677, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((480, 1) vs (512, 1)).\n"
          ]
        }
      ],
      "source": [
        "dcgan = DCGAN(100, BATCH_SIZE, (28, 28, 1))\n",
        "dcgan.train(train_dataset, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation\n",
        "\n",
        "Firstly, evaluating GAN model is hard. Unlike classification, we need to compare the generated image to a real image. But how exactly can you compare/quantify the realism of the generated image.\n",
        "\n",
        "The main 2 evaluation metrics for GAN models is:\n",
        "- **Fidelity**: Our GAN should generate _high_ quality images\n",
        "- **Diversity**: Our GAN should generate images that are inherent in the training dataset\n",
        "\n",
        "There are 2 approaches to compare the images:\n",
        "- **Pixel Distance**: The naive distance measure where we just subtract the two images' pixel value. However this approach is not reliable\n",
        "- **Feature Distance**: We use a pre-trained image classfication model and use the activation of an intermediate layer. This vector is a high level representation of our image. Computing the distancee mtric with the representation gives a stable and reliable result.\n",
        "\n",
        "## Fretchet Inception Distance (FID)\n",
        "This is one of the popular metrics to measure the feature distance. Frechet Distance is a measure of similarity between curves that takes into account the location and ordering of the points along the curves. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gpu_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
