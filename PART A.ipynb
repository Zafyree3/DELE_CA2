{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DELE ST1504 CA2\n",
        "# PART A: GAN\n",
        "\n",
        "<hr>\n",
        "\n",
        "**NAME**: Irman Zafyree, Adam Tan\n",
        "\n",
        "**ADMIN NO**: `2300546`, `2300575`\n",
        "\n",
        "**CLASS**: DAAA/FT/2B/07\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Objective:**\n",
        "\n",
        "Code a GAN model that is able to generate 260 small black-and-white images of the given dataset in 26 distinct classes.\n",
        "\n",
        "**Background:**\n",
        "A Generative Adversarial Network (GAN) is a type of deep learning model consisting of two neural networks: a generator and a discriminator. The primary purpose of a GAN is to generate new data instances that resemble the training data, through a competitive process between a generator and a discriminator. It has revolutionized the field of generative modeling and continues to be a vibrant area of research and application in artificial intelligence.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic imports\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '3'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy\n",
        "from numpy import cov\n",
        "from numpy import trace\n",
        "from numpy import iscomplexobj\n",
        "from scipy.linalg import sqrtm\n",
        "from numpy.random import randn, randint,random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from IPython import display\n",
        "\n",
        "# Tensorflow imports\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.layers import Dense, Reshape, Conv2D, LeakyReLU, Dropout, Flatten, Conv2DTranspose, BatchNormalization, Input, Embedding, Concatenate, Lambda, MaxPooling2D, UpSampling2D, Activation, GlobalAveragePooling2D, AveragePooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.train import Checkpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from keras import layers\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "dataset_path = os.environ.get(\"DATASET_PATH\")\n",
        "\n",
        "print(dataset_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    print(tf.config.experimental.get_device_details(gpu))\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "print(f\"There are {len(gpus)} GPUs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_path, header=None)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Background Research\n",
        "\n",
        "**CSV Dataset:**\n",
        "- the datatset consists of 99040 datasets\n",
        "- the first column represents the classs labels -1 to 26.\n",
        "- Remaining 784 columns are the pixel data\n",
        "\n",
        "**Classes:**\n",
        "\n",
        "Total of 27 distinct classes \n",
        "1. -1 (all black)\n",
        "2. 1 (a)\n",
        "3. 2 (b)\n",
        "4. 3 (c)\n",
        "5. 4 (d)\n",
        "6. 5 (e)\n",
        "7. 6 (f)\n",
        "8. 7 (g)\n",
        "9. 8 (h)\n",
        "10. 9 (i)\n",
        "11. 10 (j)\n",
        "12. 11 (k)\n",
        "13. 12 (l)\n",
        "14. 13 (m)\n",
        "15. 14 (n)\n",
        "16. 15 (o)\n",
        "17. 16 (p)\n",
        "18. 17 (q)\n",
        "19. 18 (r)\n",
        "20. 19 (s)\n",
        "21. 20 (t)\n",
        "22. 21 (u)\n",
        "23. 22 (v)\n",
        "24. 23 (w)\n",
        "25. 24 (x)\n",
        "26. 25 (y)\n",
        "27. 26 (z)\n",
        "\n",
        "\n",
        "**Images:**\n",
        "\n",
        "The images are of size 28x28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = df[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df.drop(df.columns[0], axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Function to display images\n",
        "def display_images(images, label):\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for index, image in enumerate(images[:20]):\n",
        "        plt.subplot(1, 20, index + 1)\n",
        "        image = np.array(image).reshape(28, 28)\n",
        "        rotated_image = np.rot90(image, k=-1)  \n",
        "        flipped_horizontal = np.fliplr(rotated_image)\n",
        "        plt.imshow(flipped_horizontal, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{label}\")\n",
        "\n",
        "# Order labels from -1 to 26\n",
        "ordered_labels = sorted(labels.unique(), key=lambda x: (x != -1, x))\n",
        "\n",
        "# Iterate through each ordered label and display 10 images\n",
        "for label in ordered_labels:\n",
        "    label_images = data[labels == label].values\n",
        "    display_images(label_images, label)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Count occurrences of each label\n",
        "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "# Print the distribution\n",
        "print(\"Class distribution:\")\n",
        "for label, count in zip(unique_labels, label_counts):\n",
        "    print(f\"Label {label}: {count} instances\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))  # Specify the figure size (optional)\n",
        "plt.bar(unique_labels, label_counts, color='skyblue')\n",
        "\n",
        "# Customize the chart\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Counts')\n",
        "plt.grid(True)  # Add gridlines for better readability (optional)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df[df[0] != -1]\n",
        "df.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_labels = df[0]\n",
        "df_data = df.drop(df.columns[0:2], axis=1)\n",
        "df_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "augmented_data = df_data.copy()\n",
        "for i in augmented_data.index:\n",
        "    pixels = augmented_data.loc[i].values\n",
        "    image = np.array(pixels).reshape(28, 28)\n",
        "    rotated_image = np.rot90(image, k=-1)  \n",
        "    flipped_horizontal = np.fliplr(rotated_image)\n",
        "    augmented_data.loc[i] = flipped_horizontal.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(augmented_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(np.array(augmented_data.loc[i]).reshape(28, 28), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def average_images_per_class(data, labels):\n",
        "\n",
        "    avg_image_df = pd.DataFrame()\n",
        "\n",
        "    # average image\n",
        "    for class_name, group in data.groupby(labels):\n",
        "        avg_image = group.mean()\n",
        "        avg_image_df[class_name] = avg_image\n",
        "\n",
        "    # Create a col of subplots\n",
        "    fig, axes = plt.subplots(5, 6, figsize=(20, 15))\n",
        "    for ax, (class_name, avg_image) in zip(axes.flatten(), avg_image_df.items()):\n",
        "        # image = avg_image.reshape(28, 28)\n",
        "        # rotated_image = np.rot90(image, k=-1)  \n",
        "        # flipped_horizontal = np.fliplr(rotated_image)\n",
        "        # ax.imshow(flipped_horizontal, cmap='gray')\n",
        "        # ax.set_title(f\"Average image for {class_name}\")\n",
        "        # ax.axis('off')  # Turn off axis\n",
        "\n",
        "        ax.imshow(np.array(avg_image).reshape(28, 28), cmap='gray')\n",
        "        ax.set_title(f\"Average image for {class_name}\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "average_images_per_class(augmented_data, df_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Research\n",
        "\n",
        "From researching online, there seem to be 3 GAN Variations, Deep Convolutional GAN (DCGAN), Conditional GAN (CGAN), Wasserstein GAN (WGAN).\n",
        "\n",
        "### Deep Convolutional GAN (DCGAN)\n",
        "\n",
        "DCGANs use deep convolutional neural networks to learn the features of the input data, which allows them to generate high-resolution images that are similar to the training data. The generator network in a DCGAN typically consists of transposed convolutional layers, while the discriminator network consists of convolutional layers. The use of convolutional layers allows DCGANs to take advantage of the spatial relationships in the input data, which results in high-quality generated images.\n",
        "\n",
        "### Conditional GAN (CGAN)\n",
        "\n",
        "CGANs that allow the user to control the generated data by adding an additional input to the generator network. This input specifies the desired characteristics of the generated data, such as the color or shape of an image. CGANs can be thought of as a combination of a GAN and a conditional generative model, where the generator and discriminator are trained to take into account both the input data and the condition. This allows CGANs to generate data that is more in line with the user’s expectations.\n",
        "\n",
        "### Wasserstein GAN (WGAN)\n",
        "\n",
        "WGANs address some of the stability issues that are commonly encountered when training GANs. WGANs use the Wasserstein distance metric to evaluate the quality of the generated data, which has been shown to provide improved stability during training compared to other metrics. The Wasserstein distance metric measures the earth mover’s distance between the generated data and the training data, which provides a robust measure of the quality of the generated data. WGANs have been shown to be particularly effective for generating high-quality images.\n",
        "\n",
        "We will be using all these 3 GAN and compare them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation\n",
        "\n",
        "Firstly, evaluating GAN model is hard. Unlike classification, we need to compare the generated image to a real image. But how exactly can you compare/quantify the realism of the generated image.\n",
        "\n",
        "The main 2 evaluation metrics for GAN models is:\n",
        "- **Fidelity**: Our GAN should generate _high_ quality images\n",
        "- **Diversity**: Our GAN should generate images that are inherent in the training dataset\n",
        "\n",
        "There are 2 approaches to compare the images:\n",
        "- **Pixel Distance**: The naive distance measure where we just subtract the two images' pixel value. However this approach is not reliable\n",
        "- **Feature Distance**: We use a pre-trained image classfication model and use the activation of an intermediate layer. This vector is a high level representation of our image. Computing the distancee mtric with the representation gives a stable and reliable result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fretchet Inception Distance (FID)\n",
        "This is one of the popular metrics to measure the feature distance. Frechet Distance is a measure of similarity between curves that takes into account the location and ordering of the points along the curves. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating the Base VAE Model to train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VariationalAutoencoder(keras.Model):\n",
        "    def __init__(self, latent_dim, input_shape):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = self.build_encoder(input_shape)\n",
        "        self.decoder = self.build_decoder()\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    def build_encoder(self, input_shape):\n",
        "        inputs = keras.Input(shape=input_shape)\n",
        "        x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
        "        x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dense(16, activation=\"relu\")(x)\n",
        "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(x)\n",
        "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(x)\n",
        "        z = layers.Lambda(self.sampling, output_shape=(self.latent_dim,), name='z')([z_mean, z_log_var])\n",
        "        model = keras.Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "        return model\n",
        "\n",
        "    def build_decoder(self):\n",
        "        latent_inputs = keras.Input(shape=(self.latent_dim,))\n",
        "        x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "        x = layers.Reshape((7, 7, 64))(x)\n",
        "        x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "        decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "        model = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "        return model\n",
        "\n",
        "    def sampling(self, args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction),\n",
        "                    axis=(1, 2),\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def plot_latent_space(self, n=30, figsize=15):\n",
        "        # display a n*n 2D manifold of digits\n",
        "        digit_size = 28\n",
        "        scale = 1.0\n",
        "        figure = np.zeros((digit_size * n, digit_size * n))\n",
        "        # linearly spaced coordinates corresponding to the 2D plot\n",
        "        # of digit classes in the latent space\n",
        "        grid_x = np.linspace(-scale, scale, n)\n",
        "        grid_y = np.linspace(-scale, scale, n)[::-1]\n",
        "\n",
        "        for i, yi in enumerate(grid_y):\n",
        "            for j, xi in enumerate(grid_x):\n",
        "                z_sample = np.array([[xi, yi]])\n",
        "                x_decoded = self.decoder.predict(z_sample, verbose=0)\n",
        "                digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "                figure[\n",
        "                    i * digit_size : (i + 1) * digit_size,\n",
        "                    j * digit_size : (j + 1) * digit_size,\n",
        "                ] = digit\n",
        "\n",
        "        plt.figure(figsize=(figsize, figsize))\n",
        "        start_range = digit_size // 2\n",
        "        end_range = n * digit_size + start_range\n",
        "        pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "        sample_range_x = np.round(grid_x, 1)\n",
        "        sample_range_y = np.round(grid_y, 1)\n",
        "        plt.xticks(pixel_range, sample_range_x)\n",
        "        plt.yticks(pixel_range, sample_range_y)\n",
        "        plt.xlabel(\"z[0]\")\n",
        "        plt.ylabel(\"z[1]\")\n",
        "        plt.imshow(figure, cmap=\"Greys_r\")\n",
        "        plt.show()\n",
        "    \n",
        "    def save_models(self, epoch):\n",
        "        self.encoder.save(f'vae_encoder_{epoch}.h5')\n",
        "        self.decoder.save(f'vae_decoder_{epoch}.h5')\n",
        "\n",
        "# Example usage remains the same\n",
        "        \n",
        "    def restore_weights(self, epoch):\n",
        "        self.encoder = keras.models.load_model(\"vae_encoder_500.h5\", custom_objects={'sampling': self.sampling})\n",
        "        self.decoder = keras.models.load_model(\"vae_decoder_500.h5\", custom_objects={'sampling': self.sampling})\n",
        "        \n",
        "    def generate_images(self, n=10):\n",
        "        latent_points = np.random.normal(size=(n, self.latent_dim))\n",
        "        generated_images = self.decoder.predict(latent_points)\n",
        "        return generated_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "vae_data = np.array(augmented_data)\n",
        "vae_data = np.expand_dims(vae_data, -1).astype(\"float32\") / 255\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10)\n",
        "\n",
        "vaeNew = VariationalAutoencoder(2, (28, 28, 1))\n",
        "'''\n",
        "vaeNew.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
        "vaeNew.fit(vae_data, epochs=500, batch_size=64, callbacks=[early_stopping])\n",
        "vaeNew.plot_latent_space()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vaeNew.restore_weights(500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Doing FID calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate frechet inception distance\n",
        "def calculate_fid(encoder, real_data, fake_data):\n",
        "\n",
        "\tz_mean_act1, _, _ = encoder.predict(real_data, verbose=0)\n",
        "\tz_mean_act2, _, _ = encoder.predict(fake_data, verbose=0)\n",
        "\t# calculate mean and covariance statistics\n",
        "\tmu1, sigma1 = z_mean_act1.mean(axis=0), cov(z_mean_act1, rowvar=False)\n",
        "\tmu2, sigma2 = z_mean_act2.mean(axis=0), cov(z_mean_act2, rowvar=False)\n",
        "\t# calculate sum squared difference between means\n",
        "\tssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
        "\t# calculate sqrt of product between cov\n",
        "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
        "\t# check and correct imaginary numbers from sqrt\n",
        "\tif iscomplexobj(covmean):\n",
        "\t\tcovmean = covmean.real\n",
        "\t# calculate score\n",
        "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "\treturn fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_path, header=None)\n",
        "\n",
        "train_labels = df[0]\n",
        "train_data = df.drop(0, axis=1)\n",
        "augmented_data = []\n",
        "for i in train_data.index:\n",
        "    pixels = train_data.loc[i].values\n",
        "    image = np.array(pixels).reshape(28,28)\n",
        "    rotated_image = np.rot90(image, k=-1)\n",
        "    flipped_horizontal = np.fliplr(rotated_image)\n",
        "    augmented_data.append(flipped_horizontal)\n",
        "\n",
        "train_data = np.array(augmented_data)\n",
        "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1).astype('float32')\n",
        "train_data = (train_data - 127.5) / 127.5 # Normalize the images to [-1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test the calculation of fid\n",
        "real_data = train_data[:100]\n",
        "fake_data = randint(0, 255, 28 * 28 * 100)\n",
        "fake_data = fake_data.reshape((100, 28, 28, 1)).astype('float32')\n",
        "calculate_fid(vaeNew.encoder, real_data, fake_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inception Score\n",
        "The Inception Score, or IS for short, is an objective metric for evaluating the quality of generated images, specifically synthetic images output by generative adversarial network models. However one issue is that it require a classification model to use. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# classifier\n",
        "class Classifier():\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = self.build_model()\n",
        "        self.callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "        model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def train(self, x_train, y_train, x_test, y_test, epochs=10, batch_size=128):\n",
        "        self.model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=self.callbacks)\n",
        "\n",
        "    def evaluate(self, x_test, y_test):\n",
        "        return self.model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    def save_model(self, model_name):\n",
        "        self.model.save(model_name)\n",
        "\n",
        "    def load_model(self, model_name):\n",
        "        self.model = keras.models.load_model(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def augment_images(data):\n",
        "    augmented_data = []\n",
        "    for i in data.index:\n",
        "        pixels = data.loc[i].values\n",
        "        image = np.array(pixels).reshape(28, 28)\n",
        "        rotated_image = np.rot90(image, k=-1)  \n",
        "        flipped_horizontal = np.fliplr(rotated_image)\n",
        "        augmented_data.append(flipped_horizontal)\n",
        "    return augmented_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier = Classifier((28, 28, 1), 26)\n",
        "\n",
        "df = pd.read_csv(dataset_path, header=None)\n",
        "df = df[df[0] != -1]\n",
        "train, test = train_test_split(df, test_size=0.2)\n",
        "\n",
        "train_labels = train[0]\n",
        "train_labels = list(map(lambda x: x - 1, train_labels))\n",
        "train_labels = to_categorical(train_labels, num_classes=26)\n",
        "train_data = train.drop(train.columns[0], axis=1)\n",
        "train_data = augment_images(train_data)\n",
        "train_data = np.expand_dims(train_data, -1).astype(\"float32\") / 255\n",
        "\n",
        "test_labels = test[0]\n",
        "test_labels = list(map(lambda x: x - 1, test_labels))\n",
        "test_labels = to_categorical(test_labels, num_classes=26)\n",
        "test_data = test.drop(test.columns[0], axis=1)\n",
        "test_data = augment_images(test_data)\n",
        "test_data = np.expand_dims(test_data, -1).astype(\"float32\") / 255\n",
        "\n",
        "classifier.load_model(\"classifier.h5\")\n",
        "#classifier.train(train_data, train_labels, test_data, test_labels, epochs=400, batch_size=64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "too hard i give up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_kl (p, q):\n",
        "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
        "\n",
        "def estimate_distribution(data, bins=100):\n",
        "    hist, bin_edges = np.histogram(data, bins=bins, density=True)\n",
        "    return hist, bin_edges\n",
        "\n",
        "def kl_divergence(real_data, fake_data, bins=100):\n",
        "    hist_real, bin_edges_real = estimate_distribution(real_data, bins)\n",
        "    hist_fake, bin_edges_fake = estimate_distribution(fake_data, bins)\n",
        "    kl = calculate_kl(hist_real, hist_fake)\n",
        "    return kl\n",
        "\n",
        "def calculate_jsd(real_data, fake_data, bins=100):\n",
        "    hist_real, bin_edges_real = estimate_distribution(real_data, bins)\n",
        "    hist_fake, bin_edges_fake = estimate_distribution(fake_data, bins)\n",
        "    hist_avg = 0.5 * (hist_real + hist_fake)\n",
        "    jsd = 0.5 * (calculate_kl(hist_real, hist_avg) + calculate_kl(hist_fake, hist_avg))\n",
        "    return jsd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_data_kl = train_data[:100]\n",
        "fake_data_kl = randint(0, 255, 28 * 28 * 100)\n",
        "fake_data_kl = fake_data_kl.reshape((100, 28, 28, 1)).astype('float32')\n",
        "print(f\"KL Real vs Real: {kl_divergence(real_data_kl, real_data_kl)}\")\n",
        "print(f\"KL Real vs Fake: {kl_divergence(real_data_kl, fake_data_kl)}\")\n",
        "\n",
        "real_data_jsd = train_data[:100]\n",
        "fake_data_jsd = randint(0, 255, 28 * 28 * 100)\n",
        "fake_data_jsd = fake_data_jsd.reshape((100, 28, 28, 1)).astype('float32')\n",
        "print(f\"JSD Real vs Real: {calculate_jsd(real_data_jsd, real_data_jsd)}\")\n",
        "print(f\"JSD Real vs Fake: {calculate_jsd(real_data_jsd, fake_data_jsd)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background on GAN\n",
        "\n",
        "A generative adversarial network (GAN) has two parts:\n",
        "- Generator\n",
        "  - learns to create \"real\" data\n",
        "  - generated data becomes the fake training examples\n",
        "- Discriminator\n",
        "  - distinguish between real and fake data\n",
        "  - penalizes the generator if discriminator detects the fake generated data\n",
        "\n",
        "When the training begins, the generator fake data is easily detectable by the discriminator. However, if the generator training goes well, the fake data becomes indistinguishable to the discriminator.\n",
        "\n",
        "Below is an example of how the generator improves over time:\n",
        "1. ![Beginning](https://developers.google.com/static/machine-learning/gan/images/bad_gan.svg)\n",
        "2. ![Middle](https://developers.google.com/static/machine-learning/gan/images/ok_gan.svg)\n",
        "3. ![End](https://developers.google.com/static/machine-learning/gan/images/good_gan.svg)\n",
        "\n",
        "So this is how the entire system will look.\n",
        "![Entire GAN system](https://developers.google.com/static/machine-learning/gan/images/gan_diagram.svg)\n",
        "\n",
        "### Discriminator\n",
        "The main goal of the discriminator is to distinguish real data from the fake generated data. It could use any network architecture appropriate to the type of data it's classifying.\n",
        "\n",
        "The discriminator training data come from 2 sources.\n",
        "- Real data comes from the initial dataset that you would like to generate like, in this case, it would be out `emnist-letter.csv`\n",
        "- Fake data comes from the generator.\n",
        "\n",
        "The discriminator trains by:\n",
        "1. Classifying real and fake\n",
        "2. The loss penalizes the discriminator for misclassifying\n",
        "3. The discriminator updates it weights through backpropagation\n",
        "\n",
        "### Generator\n",
        "The main goal of the generator is to trick the discriminator into thinking it fake data is real.\n",
        "\n",
        "The generator has a random input. It is usually random noise, and the generator turns the noise into meaningful data. Experiments suggest that the distribution of the noise doesn't matter much, so we can choose something that's easy to sample from, like a uniform distribution.\n",
        "\n",
        "The generators trains by:\n",
        "1. Sample noise\n",
        "2. Produce output\n",
        "3. Get discriminator to classify\n",
        "4. Get the loss from the discriminator\n",
        "5. Backpropagate through the generator network\n",
        "6. Updates the weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_path, header=None)\n",
        "\n",
        "train_labels = df[0]\n",
        "train_data = df.drop(0, axis=1)\n",
        "augmented_data = []\n",
        "for i in train_data.index:\n",
        "    pixels = train_data.loc[i].values\n",
        "    image = np.array(pixels).reshape(28,28)\n",
        "    rotated_image = np.rot90(image, k=-1)\n",
        "    flipped_horizontal = np.fliplr(rotated_image)\n",
        "    augmented_data.append(flipped_horizontal)\n",
        "\n",
        "train_data = np.array(augmented_data)\n",
        "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1).astype('float32')\n",
        "train_data = (train_data - 127.5) / 127.5 # Normalize the images to [-1, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from [-1,1 ] to [0, 1]\n",
        "def denormalize_image(image):\n",
        "    image = (image + 1) / 2\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DCGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DCGAN:\n",
        "    # Define the standalone discriminator model\n",
        "    def define_discriminator(self,in_shape=(28,28,1)):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # Define the standalone generator model\n",
        "    def define_generator(self):\n",
        "        latent_dim = self.latent_dim\n",
        "        model = Sequential()\n",
        "        n_nodes = 128 * 7 * 7\n",
        "        model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((7, 7, 128)))\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Conv2D(1, (3,3), activation='tanh', padding='same'))\n",
        "        return model\n",
        "\n",
        "    # Define the combined generator and discriminator model, for updating the generator\n",
        "    def define_gan(self):\n",
        "        d_model = self.d_model\n",
        "        g_model = self.g_model\n",
        "        d_model.trainable = False\n",
        "        model = Sequential()\n",
        "        model.add(g_model)\n",
        "        model.add(d_model)\n",
        "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "        return model\n",
        "\n",
        "    # Select real samples\n",
        "    def generate_real_samples(self, n_samples):\n",
        "        dataset = self.dataset\n",
        "        ix = randint(0, dataset.shape[0], n_samples)\n",
        "        X = dataset[ix]\n",
        "        y = np.ones((n_samples, 1))\n",
        "        return X, y\n",
        "\n",
        "    # Generate points in latent space as input for the generator\n",
        "    def generate_latent_points(self, n_samples):\n",
        "        latent_dim = self.latent_dim\n",
        "        x_input = randn(latent_dim * n_samples)\n",
        "        x_input = x_input.reshape(n_samples, latent_dim)\n",
        "        return x_input\n",
        "\n",
        "    # Use the generator to generate n fake examples, with class labels\n",
        "    def generate_fake_samples(self, n_samples, seeded=False):\n",
        "        g_model = self.g_model\n",
        "        latent_dim = self.latent_dim\n",
        "        x_input = self.generate_latent_points(n_samples)\n",
        "        if seeded:\n",
        "            x_input = self.seeded_latent_points\n",
        "        X = g_model.predict(x_input,verbose=0)\n",
        "        y = np.zeros((n_samples, 1))\n",
        "        return X, y\n",
        "\n",
        "    # Train the generator and discriminator\n",
        "    def train(self,n_epochs=200, n_batch=128, start_epoch=0, threshold=0.5):\n",
        "        g_model = self.g_model\n",
        "        d_model = self.d_model\n",
        "        gan_model = self.gan_model\n",
        "        dataset = self.dataset\n",
        "        latent_dim = self.latent_dim\n",
        "        self.epochs = n_epochs\n",
        "        bat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "        half_batch = int(n_batch / 2)\n",
        "        old_g_loss = 0\n",
        "\n",
        "        for i in range(start_epoch, n_epochs):\n",
        "            j = 0\n",
        "            for j in tqdm(range(bat_per_epo), desc=f'Epoch {i + 1}/{n_epochs}', total=bat_per_epo):\n",
        "                X_real, y_real = self.generate_real_samples(half_batch)\n",
        "                #print(1)\n",
        "                d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "                X_fake, y_fake = self.generate_fake_samples(half_batch)\n",
        "                #print(2)\n",
        "                d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "                X_gan = self.generate_latent_points(n_batch)\n",
        "                y_gan = np.ones((n_batch, 1))\n",
        "                g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "                #print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "\n",
        "            # Stopping condition\n",
        "            diff = abs(g_loss - old_g_loss)\n",
        "            if diff < threshold:\n",
        "                self.on_epoch_end(i, g_loss=g_loss, d_loss1=d_loss1, d_loss2=d_loss2, diff=diff)\n",
        "                break\n",
        "            old_g_loss = g_loss\n",
        "            \n",
        "            # Call the callback at the end of each epoch\n",
        "            self.on_epoch_end(i, g_loss=g_loss, d_loss1=d_loss1, d_loss2=d_loss2, diff=diff)\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None, g_loss=None, d_loss1=None, d_loss2=None, diff=None):\n",
        "        #clear output\n",
        "        display.clear_output(wait=True)\n",
        "        #print logs\n",
        "        X_real, y_real = self.generate_real_samples(self.seeded_latent_points.shape[0])\n",
        "        _, acc_real = self.d_model.evaluate(X_real, y_real, verbose=0)\n",
        "        x_fake, y_fake = self.generate_fake_samples(self.seeded_latent_points.shape[0], seeded=True)\n",
        "        _, acc_fake = self.d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "        #print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "        if (epoch + 1) % self.save_interval == 0:\n",
        "            self.save_plot(x_fake, epoch)\n",
        "            g_filename = os.path.join(self.save_dir, 'generator_model_%03d.h5' % (epoch+1))\n",
        "            self.g_model.save(g_filename)\n",
        "            d_filename = os.path.join(self.save_dir, 'discriminator_model_%03d.h5' % (epoch+1))\n",
        "            self.d_model.save(d_filename)\n",
        "        \n",
        "        # logging\n",
        "        # FID\n",
        "        fid = calculate_fid(vaeNew.encoder, X_real, x_fake)\n",
        "        x_fake_denorm = denormalize_image(x_fake)\n",
        "        x_real_denorm = denormalize_image(X_real)\n",
        "        kl = kl_divergence(x_real_denorm, x_fake_denorm)\n",
        "        jsd = calculate_jsd(x_real_denorm, x_fake_denorm)\n",
        "        self.logging[epoch] = {'acc_real': acc_real, 'acc_fake': acc_fake, 'fid': fid, 'kl': kl, 'jsd': jsd, 'g_loss': g_loss, 'd_loss1': d_loss1, 'd_loss2': d_loss2}\n",
        "\n",
        "        print(f'FID: {fid}, KL: {kl}, JSD: {jsd} | G Loss: {g_loss}, D Loss Real: {d_loss1}, D Loss Fake: {d_loss2}, Diff: {diff} | Epoch {epoch + 1}/{self.epochs}')\n",
        "        \n",
        "\n",
        "    def save_plot(self, examples, epoch, n=7):\n",
        "        examples = (examples + 1) / 2.0 # scale from [-1,1] to [0,1]\n",
        "        for i in range(min(n * n, examples.shape[0])):\n",
        "            plt.subplot(n, n, 1 + i)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(examples[i], cmap='gray')\n",
        "        filename = os.path.join(self.save_dir, 'generated_plot_e%03d.png' % (epoch+1))\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "    \n",
        "    def restore_start_training(self, epoch):\n",
        "        #dcgan_gen_img\\generator_model_076.h5\n",
        "        self.g_model = tf.keras.models.load_model(os.path.join(self.save_dir, f'generator_model_{epoch:03d}.h5'))\n",
        "        self.d_model = tf.keras.models.load_model(os.path.join(self.save_dir, f'discriminator_model_{epoch:03d}.h5'))\n",
        "        self.gan_model = self.define_gan()\n",
        "    \n",
        "    # Init the class\n",
        "    def __init__(self, latent_dim=100, dataset=None, save_interval=1, save_dir='generated_images', clear_dir=False):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_model = self.define_discriminator()\n",
        "        self.g_model = self.define_generator()\n",
        "        self.gan_model = self.define_gan()\n",
        "        self.dataset = dataset\n",
        "        self.save_interval = save_interval\n",
        "        self.save_dir = save_dir\n",
        "        self.clear_dir = clear_dir\n",
        "        self.seeded_latent_points = self.generate_latent_points(1000)\n",
        "        self.logging = {}\n",
        "        print(self.seeded_latent_points.shape)\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        if clear_dir:\n",
        "            for f in os.listdir(save_dir):\n",
        "                os.remove(os.path.join(save_dir, f))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dcgan = DCGAN(latent_dim=100, dataset=train_data, save_interval=1, save_dir='dcgan_gen_img_v2', clear_dir=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dcgan.train(n_epochs=250, n_batch=128, start_epoch=0, threshold=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CDCGAN:\n",
        "    # Define the standalone discriminator model\n",
        "    def define_discriminator(self,in_shape=(28,28,1)):\n",
        "        # label input\n",
        "        n_classes = self.n_classes\n",
        "        in_label = Input(shape=(1,))\n",
        "        li = Embedding(n_classes, 50)(in_label)\n",
        "        n_nodes = in_shape[0] * in_shape[1]\n",
        "        li = Dense(n_nodes)(li)\n",
        "        li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
        "\n",
        "        # image input\n",
        "        in_image = Input(shape=in_shape)\n",
        "\n",
        "        # concat label as a channel\n",
        "        merge = Concatenate()([in_image, li])\n",
        "\n",
        "        model = Conv2D(64, (3,3), padding='same')(merge)\n",
        "        model = LeakyReLU(alpha=0.2)(model)\n",
        "        model = Conv2D(128, (3,3), strides=(2,2), padding='same')(model)\n",
        "        model = LeakyReLU(alpha=0.2)(model)\n",
        "        model = Conv2D(128, (3,3), strides=(2,2), padding='same')(model)\n",
        "        model = LeakyReLU(alpha=0.2)(model)\n",
        "        model = Conv2D(256, (3,3), strides=(2,2), padding='same')(model)\n",
        "        model = LeakyReLU(alpha=0.2)(model)\n",
        "        model = Flatten()(model)\n",
        "        model = Dropout(0.4)(model)\n",
        "        out_layer = Dense(1, activation='sigmoid')(model)\n",
        "        model = Model([in_image, in_label], out_layer)\n",
        "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # Define the standalone generator model\n",
        "    def define_generator(self):\n",
        "        latent_dim = self.latent_dim\n",
        "\n",
        "        # label input'\n",
        "        in_label = Input(shape=(1,))\n",
        "        li = Embedding(self.n_classes, 50)(in_label)\n",
        "        n_nodes = 7 * 7\n",
        "        li = Dense(n_nodes)(li)\n",
        "        li = Reshape((7, 7, 1))(li)\n",
        "\n",
        "        # image generator input\n",
        "        in_lat = Input(shape=(latent_dim,))\n",
        "        n_nodes = 128 * 7 * 7\n",
        "        gen = Dense(n_nodes)(in_lat)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        gen = Reshape((7, 7, 128))(gen)\n",
        "\n",
        "        # merge image gen and label input\n",
        "        merge = Concatenate()([gen, li])\n",
        "\n",
        "        model = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
        "        model = LeakyReLU(alpha=0.2)(model)\n",
        "        model = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(model)\n",
        "        model = LeakyReLU(alpha=0.2)(model)\n",
        "        model = Conv2D(1, (3,3), activation='tanh', padding='same')(model)\n",
        "        model = Model([in_lat, in_label], model)\n",
        "        return model\n",
        "\n",
        "    # Define the combined generator and discriminator model, for updating the generator\n",
        "    def define_gan(self):\n",
        "        d_model = self.d_model\n",
        "        g_model = self.g_model\n",
        "        d_model.trainable = False\n",
        "        g_noise, g_label = g_model.input\n",
        "        g_output = g_model.output\n",
        "        gan_output = d_model([g_output, g_label])\n",
        "        model = Model([g_noise, g_label], gan_output)\n",
        "        opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "        return model\n",
        "\n",
        "    # Select real samples\n",
        "    def generate_real_samples(self, n_samples):\n",
        "        dataset = self.dataset\n",
        "        images, labels = dataset\n",
        "        ix = randint(0, images.shape[0], n_samples)\n",
        "        X, labels = images[ix], labels[ix]\n",
        "        y = np.ones((n_samples, 1))\n",
        "        return [X, labels], y\n",
        "\n",
        "    # Generate points in latent space as input for the generator\n",
        "    def generate_latent_points(self, n_samples):\n",
        "        latent_dim = self.latent_dim\n",
        "        n_classes = self.n_classes\n",
        "\n",
        "        x_input = randn(latent_dim * n_samples)\n",
        "        z_input = x_input.reshape(n_samples, latent_dim)\n",
        "        labels = randint(0, n_classes, n_samples)\n",
        "        return [z_input, labels]\n",
        "\n",
        "    # Use the generator to generate n fake examples, with class labels\n",
        "    def generate_fake_samples(self, n_samples, seeded=False):\n",
        "        g_model = self.g_model\n",
        "        latent_dim = self.latent_dim\n",
        "        z_input, labels_input = self.generate_latent_points(n_samples)\n",
        "        if seeded:\n",
        "            z_input, labels_input = self.seeded_latent_points\n",
        "        images = g_model.predict([z_input, labels_input], verbose=0)\n",
        "        y = np.zeros((n_samples, 1))\n",
        "        return [images, labels_input], y\n",
        "\n",
        "    def save_plot(self, examples, epoch, n=7):\n",
        "        examples = (examples + 1) / 2.0\n",
        "        for i in range(n * n):\n",
        "            plt.subplot(n, n, 1 + i)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(examples[i], cmap='gray')\n",
        "        filename = os.path.join(self.save_dir, 'generated_plot_e%03d.png' % (epoch+1))\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        #clear output\n",
        "        display.clear_output(wait=True)\n",
        "        #print logs\n",
        "        [X_real, labels_real], y_real = self.generate_real_samples(100)\n",
        "        _, acc_real = self.d_model.evaluate([X_real, labels_real], verbose=0)\n",
        "        [x_fake, labels], y_fake = self.generate_fake_samples(100, seeded=True)\n",
        "        _, acc_fake = self.d_model.evaluate([x_fake, labels], y_fake, verbose=0)\n",
        "        print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "        if (epoch + 1) % self.save_interval == 0:\n",
        "            self.save_plot(x_fake, epoch)\n",
        "            g_filename = os.path.join(self.save_dir, 'generator_model_%03d.h5' % (epoch+1))\n",
        "            self.g_model.save(g_filename)\n",
        "            d_filename = os.path.join(self.save_dir, 'discriminator_model_%03d.h5' % (epoch+1))\n",
        "            self.d_model.save(d_filename)\n",
        "\n",
        "\n",
        "    # Train the generator and discriminator\n",
        "    def train(self,n_epochs=200, n_batch=128):\n",
        "        g_model = self.g_model\n",
        "        d_model = self.d_model\n",
        "        gan_model = self.gan_model\n",
        "        dataset = self.dataset\n",
        "        latent_dim = self.latent_dim\n",
        "        bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "        half_batch = int(n_batch / 2)\n",
        "\n",
        "        for i in range(n_epochs):\n",
        "            j = 0\n",
        "            for j in tqdm(range(bat_per_epo), desc=f'Epoch {i + 1}/{n_epochs}', total=bat_per_epo):\n",
        "                [X_real, labels_real], y_real = self.generate_real_samples(half_batch)\n",
        "                d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
        "                [X_fake, labels], y_fake = self.generate_fake_samples(half_batch)\n",
        "                d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
        "                [z_input, labels_input] = self.generate_latent_points(n_batch)\n",
        "                y_gan = np.ones((n_batch, 1))\n",
        "                g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
        "                #print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "            \n",
        "            # Call the callback at the end of each epoch\n",
        "            self.on_epoch_end(i)\n",
        "            \n",
        "    # Init the class\n",
        "    def __init__(self, latent_dim=100, n_classes=10, dataset=None, save_interval=1, save_dir='generated_images', clear_dir=False):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.dataset = dataset\n",
        "        self.n_classes = n_classes\n",
        "        self.save_interval = save_interval\n",
        "        self.save_dir = save_dir\n",
        "        self.clear_dir = clear_dir\n",
        "        self.d_model = self.define_discriminator()\n",
        "        self.g_model = self.define_generator()\n",
        "        self.gan_model = self.define_gan()\n",
        "        self.seeded_latent_points = self.generate_latent_points(100)\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        if clear_dir:\n",
        "            for f in os.listdir(save_dir):\n",
        "                os.remove(os.path.join(save_dir, f))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train_dataset = (train_data, train_labels)\n",
        "#cdcgan = CDCGAN(latent_dim=100, n_classes=26, dataset=train_dataset, save_interval=3, save_dir='cdcgan_gen_img', clear_dir=True)\n",
        "#cdcgan.train(n_epochs=1000, n_batch=64)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gpu_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
